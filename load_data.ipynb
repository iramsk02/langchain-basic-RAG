{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c954753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3b4746e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is generative AI?\\nGenerative AI, sometimes called gen AI, is artificial intelligence (AI) that can create original content such as text, images, video, audio or software code in response to a userâ€™s prompt or request.\\n\\nGenerative AI relies on sophisticated machine learning models called deep learning models algorithms that simulate the learning and decision-making processes of the human brain. These models work by identifying and encoding the patterns and relationships in huge amounts of data, and then using that information to understand users' natural language requests or questions and respond with relevant new content.\\n\\nAI has been a hot technology topic for the past decade, but generative AI, and specifically the arrival of ChatGPT in 2022, has thrust AI into worldwide headlines and launched an unprecedented surge of AI innovation and adoption. Generative AI offers enormous productivity benefits for individuals and organizations, and while it also presents very real challenges and risks, businesses are forging ahead, exploring how the technology can improve their internal workflows and enrich their products and services. According to research by the management consulting firm McKinsey, one third of organizations are already using generative AI regularly in at least one business function.Â¹ Industry analyst Gartner projects more than 80% of organizations will have deployed generative AI applications or used generative AI application programming interfaces (APIs) by 2026.2\\n\\n\\nThe rise of generative AI for business\\nThe rise of generative AI for business (14:44 min)\\nHow generative AI works\\nFor the most part, generative AI operates in three phases: \\n\\nTraining, to create a foundation model that can serve as the basis of multiple gen AI applications.\\n\\nTuning, to tailor the foundation model to a specific gen AI application.\\n\\nGeneration, evaluation and retuning, to assess the gen AI application's output and continually improve its quality and accuracy.\\nTraining\\nGenerative AI begins with a foundation model, a deep learning model that serves as the basis for multiple different types of generative AI applications. The most common foundation models today are large language models (LLMs), created for text generation applications, but there are also foundation models for image generation, video generation, and sound and music generation as well as multimodal foundation models that can support several kinds content generation.\\n\\nTo create a foundation model, practitioners train a deep learning algorithm on huge volumes of raw, unstructured, unlabeled data e.g., terabytes of data culled from the internet or some other huge data source. During training, the algorithm performs and evaluates millions of â€˜fill in the blankâ€™ exercises, trying to predict the next element in a sequence e.g., the next word in a sentence, the next element in an image, the next command in a line of code and continually adjusting itself to minimize the difference between its predictions and the actual data (or â€˜correctâ€™ result).\\n\\nThe result of this training is a neural network of parameters, encoded representations of the entities, patterns and relationships in the data, that can generate content autonomously in response to inputs, or prompts.\\n\\nThis training process is compute-intensive, time-consuming and expensive: it requires thousands of clustered graphics processing units (GPUs) and weeks of processing, all of which costs millions of dollars. Open-source foundation model projects, such as Meta's Llama-2, enable gen AI developers to avoid this step and its costs.\\n\\nTuning\\nMetaphorically speaking, a foundation model is a generalist: It knows a lot about a lot of types of content, but often canâ€™t generate specific types of output with desired accuracy or fidelity. For that, the model must be tuned to a specific content generation task. This can be done in a variety of ways.\\n\\nFine tuning\\nFine tuning involves feeding the model labeled data specific to the content generation application questions or prompts the application is likely to receive, and corresponding correct answers in the desired format. For example, if a development team is trying to create a customer service chatbot, it would create hundreds or thousands of documents containing labeled customers service questions and correct answers, and then feed those documents to the model.\\n\\nFine-tuning is labor-intensive. Developers often outsource the task to companies with large data-labeling workforces.\\n\\nReinforcement learning with human feedback (RLHF)\\nIn RLHF, human users respond to generated content with evaluations the model can use to update the model for greater accuracy or relevance. Often, RLHF involves people â€˜scoringâ€™ different outputs in response to the same prompt. But it can be as simple as having people type or talk back to a chatbot or virtual assistant, correcting its output.\\n\\nGeneration, evaluation, more tuning\\nDevelopers and users continually assess the outputs of their generative AI apps, and further tune the model even as often as once a week for greater accuracy or relevance. (In contrast, the foundation model itself is updated much less frequently, perhaps every year or 18 months.)\\n\\nAnother option for improving a gen AI app's performance is retrieval augmented generation (RAG). RAG is a framework for extending the foundation model to use relevant sources outside of the training data, to supplement and refine the parameters or representations in the original model. RAG can ensure that a generative AI app always has access to the most current information. As a bonus, the additional sources accessed via RAG are transparent to users in a way that the knowledge in the original foundation model is not.\\n\\n\\nWhat is Retrieval-Augmented Generation (RAG)?\\nWhat is Retrieval-Augmented Generation (RAG)? (6:32 min)\\nGenerative AI model architectures and how they have evolved\\nTruly generative AI models deep learning models that can autonomously create content on demand have evolved over the last dozen years or so. The milestone model architectures during that period include\\n\\nVariational autoencoders (VAEs), which drove breakthroughs in image recognition, natural language processing and anomaly detection.\\n \\n\\nGenerative adversarial networks (GANs) and diffusion models, which improved the accuracy of previous applications and enabled some of the first AI solutions for photo-realistic image generation.\\n \\n\\nTransformers, the deep learning model architecture behind the foremost foundation models and generative AI solutions today.\\n\\nVariational autoencoders (VAEs)\\nAn autoencoder is a deep learning model comprising two connected neural networks: One that encodes (or compresses) a huge amount of unstructured, unlabeled training data into parameters, and another that decodes those parameters to reconstruct the content. Technically, autoencoders can generate new content, but theyâ€™re more useful for compressing data for storage or transfer, and decompressing it for use, than they are for high-quality content generation.\\n\\nIntroduced in 2013, variational autoencoders (VAEs) can encode data like an autoencoder, but decode multiple new variations of the content. By training a VAE to generate variations toward a particular goal, it can â€˜zero inâ€™ on more accurate, higher-fidelity content over time. Early VAE applications included anomaly detection (e.g., medical image analysis) and natural language generation.\\n\\nGenerative adversarial networks (GANs)\\nGANs, introduced in 2014, also comprise two neural networks: A generator, which generates new content, and a discriminator, which evaluates the accuracy and quality the generated data. These adversarial algorithms encourages the model to generate increasingly high-quality outpits.\\n\\nGANs are commonly used for image and video generation, but can generate high-quality, realistic content across various domains. They've proven particularly successful at tasks as style transfer (altering the style of an image from, say, a photo to a pencil sketch) and data augmentation (creating new, synthetic data to increase the size and diversity of a training data set).\\n\\nDiffusion models\\nAlso introduced in 2014, diffusion models work by first adding noise to the training data until itâ€™s random and unrecognizable, and then training the algorithm to iteratively diffuse the noise to reveal a desired output.\\n\\nDiffusion models take more time to train than VAEs or GANs, but ultimately offer finer-grained control over output, particularly for high-quality image generation tool. DALL-E, Open AIâ€™s image-generation tool, is driven by a diffusion model.\\n\\nTransformers\\nFirst documented in a 2017 paper published by Ashish Vaswani and others, transformers evolve the encoder-decoder paradigm to enable a big step forward in the way foundation models are trained, and in the quality and range of content they can produce. These models are at the core of most of todayâ€™s headline-making generative AI tools, including ChatGPT and GPT-4, Copilot, BERT, Bard, and Midjourney to name a few.\\n\\nTransformers use a concept called attention, determining and focusing on whatâ€™s most important about data within a sequence to;\\n\\nprocess entire sequences of data e.g., sentences instead of individual words simultaneously;\\n \\n\\ncapture the context of the data within the sequence;\\n \\n\\nencode the training data into embeddings (also called hyperparameters) that represent the data and its context.\\n\\nIn addition to enabling faster training, transformers excel at natural language processing (NLP) and natural language understanding (NLU), and can generate longer sequences of data e.g., not just answers to questions, but poems, articles or papers with greater accuracy and higher quality than other deep generative AI models. Transformer models can also be trained or tuned to use tools e.g., a spreadsheet application, HTML, a drawing program to output content in a particular format.\\n\\n\\nGenerative Models Explained\\nGenerative Models Explained (8:30 min)\\nWhat generative AI can create\\nGenerative AI can create many types of content across many different domains. \\n\\nText\\nGenerative models. especially those based on transformers, can generate coherent, contextually relevant text, everything from instructions and documentation to brochures, emails, web site copy, blogs, articles, reports, papers, and even creative writing. They can also perform repetitive or tedious writing tasks (e.g., such as drafting summaries of documents or meta descriptions of web pages), freeing writersâ€™ time for more creative, higher-value work.\\n\\nImages and video\\nImage generation such as DALL-E, Midjourney and Stable Diffusion can create realistic images or original art, and can perform style transfer, image-to-image translation and other image editing or image enhancement tasks. Emerging gen AI video tools can create animations from text prompts, and can apply special effects to existing video more quickly and cost-effectively than other methods.\\n\\nSound, speech and music\\nGenerative models can synthesize natural-sounding speech and audio content for voice-enabled AI chatbots and digital assistants, audiobook narration and other applications. The same technology can generate original music that mimics the structure and sound of professional compositions.\\n\\nSoftware code\\nGen AI can generate original code, autocomplete code snippets, translate between programming languages and summarize code functionality. It enables developers to quickly prototype, refactor, and debug applications while offering a natural language interface for coding tasks.\\n\\nDesign and art\\nGenerative AI models can generate unique works of art and design, or assist in graphic design. Applications include dynamic generation of environments, characters or avatars, and special effects for virtual simulations and video games.\\n\\nSimulations and synthetic data\\nGenerative AI models can be trained to generate synthetic data, or synthetic structures based on real or synthetic data. For example, generative AI is applied in drug discovery to generate molecular structures with desired properties, aiding in the design of new pharmaceutical compounds.\\n\\nThink Newsletter\\n\\nJoin over 100,000 subscribers who read the latest news in tech\\nStay up to date on the most importantâ€”and intriguingâ€”industry trends on AI, automation, data and beyond with the Think newsletter. See the IBM Privacy Statement.\\n\\nBusiness email\\njohndoe@yourdomain.com\\nSubscribe\\n\\nBenefits of generative AI\\nThe obvious, overarching benefit of generative AI is greater efficiency. Because it can generate content and answers on demand, gen AI has the potential to accelerate or automate labor-intensive tasks, cut costs, and free employees time for higher-value work.\\n\\nBut generative AI offers several other benefits for indivuduals and organizations.\\n\\nEnhanced creativity\\nGen AI tools can inspire creativity through automated brainstorming, generating multiple novel versions of content. These variations can also serve as starting points or references that help writers, artists, designers and other creators plow through creative blocks.\\n\\nImproved (and faster) decision-making\\nGenerative AI excels at analyzing large datasets, identifying patterns and extracting meaningful insights, then generating hypotheses and recommendations based on those insights to support executives, analysts, researchers and other professionals in making smarter, data-driven decisions.\\n\\nDynamic personalization\\nIn applications like recommendation systems and content creation, generative AI can analyze user preferences and history and generate personalized content in real time, leading to a more tailored and engaging user experience.\\n\\nConstant availability\\nGenerative AI operates continuously without fatigue, providing around-the-clock availability for tasks like customer support chatbots and automated responses.\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=TextLoader(\"data.txt\")\n",
    "data=loader.load()\n",
    "# data=data.page_content\n",
    "data=data[0].page_content\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cf6c709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b87341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_data=splitter.split_text(data)\n",
    "# splitted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a34c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OllamaEmbeddings(model=\"nomic-embed-text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9c006703",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data=embeddings.embed_documents(splitted_data)\n",
    "# embedded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3d81008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=list(zip(splitted_data, embedded_data)),\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5fa76064",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=embeddings.embed_query(\"what is generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0bd53a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=vector_store.similarity_search_with_score_by_vector(query)\n",
    "# ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ae85347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"gemma3:latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0a15f633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is generative AI?\\nGenerative AI, sometimes called gen AI, is artificial intelligence (AI) that can create original content such as text, images, video, audio or software code in response to a userâ€™s prompt or request.\\n\\nGenerative AI relies on sophisticated machine learning models called deep learning models algorithms that simulate the learning and decision-making processes of the human brain. These models work by identifying and encoding the patterns and relationships in huge amounts of data, and then using that information to understand users' natural language requests or questions and respond with relevant new content.\\n\\nAI has been a hot technology topic for the past decade, but generative AI, and specifically the arrival of ChatGPT in 2022, has thrust AI into worldwide headlines and launched an unprecedented surge of AI innovation and adoption. Generative AI offers enormous productivity benefits for individuals and organizations, and while it also presents very real challenges and risks, businesses are forging ahead, exploring how the technology can improve their internal workflows and enrich their products and services. According to research by the management consulting firm McKinsey, one third of organizations are already using generative AI regularly in at least one business function.Â¹ Industry analyst Gartner projects more than 80% of organizations will have deployed generative AI applications or used generative AI application programming interfaces (APIs) by 2026.2\\n\\n\\nThe rise of generative AI for business\\nThe rise of generative AI for business (14:44 min)\\nHow generative AI works\\nFor the most part, generative AI operates in three phases: \\n\\nTraining, to create a foundation model that can serve as the basis of multiple gen AI applications.\\n\\nTuning, to tailor the foundation model to a specific gen AI application.\\n\\nGeneration, evaluation and retuning, to assess the gen AI application's output and continually improve its quality and accuracy.\\nTraining\\nGenerative AI begins with a foundation model, a deep learning model that serves as the basis for multiple different types of generative AI applications. The most common foundation models today are large language models (LLMs), created for text generation applications, but there are also foundation models for image generation, video generation, and sound and music generation as well as multimodal foundation models that can support several kinds content generation.\\n\\nTo create a foundation model, practitioners train a deep learning algorithm on huge volumes of raw, unstructured, unlabeled data e.g., terabytes of data culled from the internet or some other huge data source. During training, the algorithm performs and evaluates millions of â€˜fill in the blankâ€™ exercises, trying to predict the next element in a sequence e.g., the next word in a sentence, the next element in an image, the next command in a line of code and continually adjusting itself to minimize the difference between its predictions and the actual data (or â€˜correctâ€™ result).\\n\\nThe result of this training is a neural network of parameters, encoded representations of the entities, patterns and relationships in the data, that can generate content autonomously in response to inputs, or prompts.\\n\\nThis training process is compute-intensive, time-consuming and expensive: it requires thousands of clustered graphics processing units (GPUs) and weeks of processing, all of which costs millions of dollars. Open-source foundation model projects, such as Meta's Llama-2, enable gen AI developers to avoid this step and its costs.\\n\\nTuning\\nMetaphorically speaking, a foundation model is a generalist: It knows a lot about a lot of types of content, but often canâ€™t generate specific types of output with desired accuracy or fidelity. For that, the model must be tuned to a specific content generation task. This can be done in a variety of ways.\\n\\nFine tuning\\nFine tuning involves feeding the model labeled data specific to the content generation application questions or prompts the application is likely to receive, and corresponding correct answers in the desired format. For example, if a development team is trying to create a customer service chatbot, it would create hundreds or thousands of documents containing labeled customers service questions and correct answers, and then feed those documents to the model.\\n\\nFine-tuning is labor-intensive. Developers often outsource the task to companies with large data-labeling workforces.\\n\\nReinforcement learning with human feedback (RLHF)\\nIn RLHF, human users respond to generated content with evaluations the model can use to update the model for greater accuracy or relevance. Often, RLHF involves people â€˜scoringâ€™ different outputs in response to the same prompt. But it can be as simple as having people type or talk back to a chatbot or virtual assistant, correcting its output.\\n\\nGeneration, evaluation, more tuning\\nDevelopers and users continually assess the outputs of their generative AI apps, and further tune the model even as often as once a week for greater accuracy or relevance. (In contrast, the foundation model itself is updated much less frequently, perhaps every year or 18 months.)\\n\\nAnother option for improving a gen AI app's performance is retrieval augmented generation (RAG). RAG is a framework for extending the foundation model to use relevant sources outside of the training data, to supplement and refine the parameters or representations in the original model. RAG can ensure that a generative AI app always has access to the most current information. As a bonus, the additional sources accessed via RAG are transparent to users in a way that the knowledge in the original foundation model is not.\\n\\n\\nWhat is Retrieval-Augmented Generation (RAG)?\\nWhat is Retrieval-Augmented Generation (RAG)? (6:32 min)\\nGenerative AI model architectures and how they have evolved\\nTruly generative AI models deep learning models that can autonomously create content on demand have evolved over the last dozen years or so. The milestone model architectures during that period include\\n\\nVariational autoencoders (VAEs), which drove breakthroughs in image recognition, natural language processing and anomaly detection.\\n \\n\\nGenerative adversarial networks (GANs) and diffusion models, which improved the accuracy of previous applications and enabled some of the first AI solutions for photo-realistic image generation.\\n \\n\\nTransformers, the deep learning model architecture behind the foremost foundation models and generative AI solutions today.\\n\\nVariational autoencoders (VAEs)\\nAn autoencoder is a deep learning model comprising two connected neural networks: One that encodes (or compresses) a huge amount of unstructured, unlabeled training data into parameters, and another that decodes those parameters to reconstruct the content. Technically, autoencoders can generate new content, but theyâ€™re more useful for compressing data for storage or transfer, and decompressing it for use, than they are for high-quality content generation.\\n\\nIntroduced in 2013, variational autoencoders (VAEs) can encode data like an autoencoder, but decode multiple new variations of the content. By training a VAE to generate variations toward a particular goal, it can â€˜zero inâ€™ on more accurate, higher-fidelity content over time. Early VAE applications included anomaly detection (e.g., medical image analysis) and natural language generation.\\n\\nGenerative adversarial networks (GANs)\\nGANs, introduced in 2014, also comprise two neural networks: A generator, which generates new content, and a discriminator, which evaluates the accuracy and quality the generated data. These adversarial algorithms encourages the model to generate increasingly high-quality outpits.\\n\\nGANs are commonly used for image and video generation, but can generate high-quality, realistic content across various domains. They've proven particularly successful at tasks as style transfer (altering the style of an image from, say, a photo to a pencil sketch) and data augmentation (creating new, synthetic data to increase the size and diversity of a training data set).\\n\\nDiffusion models\\nAlso introduced in 2014, diffusion models work by first adding noise to the training data until itâ€™s random and unrecognizable, and then training the algorithm to iteratively diffuse the noise to reveal a desired output.\\n\\nDiffusion models take more time to train than VAEs or GANs, but ultimately offer finer-grained control over output, particularly for high-quality image generation tool. DALL-E, Open AIâ€™s image-generation tool, is driven by a diffusion model.\\n\\nTransformers\\nFirst documented in a 2017 paper published by Ashish Vaswani and others, transformers evolve the encoder-decoder paradigm to enable a big step forward in the way foundation models are trained, and in the quality and range of content they can produce. These models are at the core of most of todayâ€™s headline-making generative AI tools, including ChatGPT and GPT-4, Copilot, BERT, Bard, and Midjourney to name a few.\\n\\nTransformers use a concept called attention, determining and focusing on whatâ€™s most important about data within a sequence to;\\n\\nprocess entire sequences of data e.g., sentences instead of individual words simultaneously;\\n \\n\\ncapture the context of the data within the sequence;\\n \\n\\nencode the training data into embeddings (also called hyperparameters) that represent the data and its context.\\n\\nIn addition to enabling faster training, transformers excel at natural language processing (NLP) and natural language understanding (NLU), and can generate longer sequences of data e.g., not just answers to questions, but poems, articles or papers with greater accuracy and higher quality than other deep generative AI models. Transformer models can also be trained or tuned to use tools e.g., a spreadsheet application, HTML, a drawing program to output content in a particular format.\\n\\n\\nGenerative Models Explained\\nGenerative Models Explained (8:30 min)\\nWhat generative AI can create\\nGenerative AI can create many types of content across many different domains. \\n\\nText\\nGenerative models. especially those based on transformers, can generate coherent, contextually relevant text, everything from instructions and documentation to brochures, emails, web site copy, blogs, articles, reports, papers, and even creative writing. They can also perform repetitive or tedious writing tasks (e.g., such as drafting summaries of documents or meta descriptions of web pages), freeing writersâ€™ time for more creative, higher-value work.\\n\\nImages and video\\nImage generation such as DALL-E, Midjourney and Stable Diffusion can create realistic images or original art, and can perform style transfer, image-to-image translation and other image editing or image enhancement tasks. Emerging gen AI video tools can create animations from text prompts, and can apply special effects to existing video more quickly and cost-effectively than other methods.\\n\\nSound, speech and music\\nGenerative models can synthesize natural-sounding speech and audio content for voice-enabled AI chatbots and digital assistants, audiobook narration and other applications. The same technology can generate original music that mimics the structure and sound of professional compositions.\\n\\nSoftware code\\nGen AI can generate original code, autocomplete code snippets, translate between programming languages and summarize code functionality. It enables developers to quickly prototype, refactor, and debug applications while offering a natural language interface for coding tasks.\\n\\nDesign and art\\nGenerative AI models can generate unique works of art and design, or assist in graphic design. Applications include dynamic generation of environments, characters or avatars, and special effects for virtual simulations and video games.\\n\\nSimulations and synthetic data\\nGenerative AI models can be trained to generate synthetic data, or synthetic structures based on real or synthetic data. For example, generative AI is applied in drug discovery to generate molecular structures with desired properties, aiding in the design of new pharmaceutical compounds.\\n\\nThink Newsletter\\n\\nJoin over 100,000 subscribers who read the latest news in tech\\nStay up to date on the most importantâ€”and intriguingâ€”industry trends on AI, automation, data and beyond with the Think newsletter. See the IBM Privacy Statement.\\n\\nBusiness email\\njohndoe@yourdomain.com\\nSubscribe\\n\\nBenefits of generative AI\\nThe obvious, overarching benefit of generative AI is greater efficiency. Because it can generate content and answers on demand, gen AI has the potential to accelerate or automate labor-intensive tasks, cut costs, and free employees time for higher-value work.\\n\\nBut generative AI offers several other benefits for indivuduals and organizations.\\n\\nEnhanced creativity\\nGen AI tools can inspire creativity through automated brainstorming, generating multiple novel versions of content. These variations can also serve as starting points or references that help writers, artists, designers and other creators plow through creative blocks.\\n\\nImproved (and faster) decision-making\\nGenerative AI excels at analyzing large datasets, identifying patterns and extracting meaningful insights, then generating hypotheses and recommendations based on those insights to support executives, analysts, researchers and other professionals in making smarter, data-driven decisions.\\n\\nDynamic personalization\\nIn applications like recommendation systems and content creation, generative AI can analyze user preferences and history and generate personalized content in real time, leading to a more tailored and engaging user experience.\\n\\nConstant availability\\nGenerative AI operates continuously without fatigue, providing around-the-clock availability for tasks like customer support chatbots and automated responses.\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context=data\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4b91dae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Generative AI, sometimes called gen AI, is artificial intelligence (AI) that can create original content such as text, images, video, audio or software code in response to a user’s prompt or request. It relies on sophisticated machine learning models called deep learning models algorithms that simulate the learning and decision-making processes of the human brain. These models work by identifying and encoding the patterns and relationships in huge amounts of data, and then using that information to understand users' natural language requests or questions and respond with relevant new content.\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=f\"\"\"\n",
    "Answer the question using only the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "What is generative AI?\n",
    "\"\"\"\n",
    "res=llm.invoke(prompt)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
